{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will be to analyse the EPS data scraped by DataAssembly/EPS/parse_10q_filings.py\n",
    "### first analysis is to see how many blanks we have (can I do the rest by hand or not)\n",
    "### second will be to look for outliers - these will be to look for instances where the LLM screwed up. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for news.db (e.g how many articles are there for each ticker)\n",
    "import sys, os\n",
    "notebook_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, \"../..\")))\n",
    "import config\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../\"+config.QUARTERLY_EPS_DATA_CSV)\n",
    "type(df)\n",
    "df_quarterly = df[df['Form Type'] == '10-Q (Quarterly report)'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_null_eps_10q_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find rows where all EPS columns are null and the form type is '10-Q'.\n",
    "    Returns a DataFrame with index, Ticker, and AccessionNumber.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "\n",
    "\n",
    "    mask = (\n",
    "        (df['quarterly_raw_eps'].isnull()) &\n",
    "        (df['quarterly_diluted_eps'].isnull()) &\n",
    "        (df['annual_raw_eps'].isnull()) &\n",
    "        (df['annual_diluted_eps'].isnull())\n",
    "    )\n",
    "\n",
    "    result_df = df.loc[mask, ['Ticker', 'Accession Number']].copy()\n",
    "    print(f\"Total of {initial_count} rows.\")\n",
    "    print(f\"[INFO] Found {len(result_df)} rows with all EPS values missing for 10-Q filings. ({len(result_df)*100/initial_count}%)\")\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 6163 rows.\n",
      "[INFO] Found 138 rows with all EPS values missing for 10-Q filings. (2.2391692357618043%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Accession Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>TER</td>\n",
       "      <td>0001193125-17-255654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>WDC</td>\n",
       "      <td>0000106040-24-000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>WDC</td>\n",
       "      <td>0000106040-24-000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>WDC</td>\n",
       "      <td>0000106040-24-000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>WDC</td>\n",
       "      <td>0000106040-23-000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>WDC</td>\n",
       "      <td>0000106040-23-000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>WDC</td>\n",
       "      <td>0000106040-23-000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>WDC</td>\n",
       "      <td>0000106040-21-000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>ROST</td>\n",
       "      <td>0001206774-12-002608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0001564590-18-019254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker      Accession Number\n",
       "178    TER  0001193125-17-255654\n",
       "260    WDC  0000106040-24-000040\n",
       "261    WDC  0000106040-24-000022\n",
       "262    WDC  0000106040-24-000014\n",
       "263    WDC  0000106040-23-000034\n",
       "264    WDC  0000106040-23-000017\n",
       "265    WDC  0000106040-23-000010\n",
       "269    WDC  0000106040-21-000053\n",
       "506   ROST  0001206774-12-002608\n",
       "799   TSLA  0001564590-18-019254"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df = find_null_eps_10q_rows(df=df_quarterly)\n",
    "missing_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "   Ticker  MissingCount\n",
      "0      BX            23\n",
      "1     TEL            22\n",
      "2     KKR            20\n",
      "3    TSLA            16\n",
      "4      MS            11\n",
      "5     WDC             7\n",
      "6    MCHP             7\n",
      "7     HAS             6\n",
      "8     BAC             5\n",
      "9    ULTA             4\n",
      "10    AMD             4\n",
      "11     EG             3\n",
      "12    PGR             2\n",
      "13   BKNG             2\n",
      "14    AJG             2\n",
      "15   ERIE             1\n",
      "16   ROST             1\n",
      "17    PTC             1\n",
      "18    TER             1\n"
     ]
    }
   ],
   "source": [
    "ticker_missing_counts_df = missing_df['Ticker'].value_counts().reset_index()\n",
    "ticker_missing_counts_df.columns = ['Ticker', 'MissingCount']\n",
    "print(len(ticker_missing_counts_df))\n",
    "print(ticker_missing_counts_df)\n",
    "# Tickers with no articles:\n",
    "# [TEL,MCHP,BAC,EG,AJG(7),PTC]\n",
    "# 22 + 7 + 7 + 3 + 2 + 1 = 42\n",
    "# 140 - 42 = 98\n",
    "# I can do this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_eps_anomalies(df: pd.DataFrame, threshold_pct: float = 5.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flags EPS rows where:\n",
    "    - Raw and diluted EPS have opposite signs\n",
    "    - Absolute percentage difference exceeds threshold AND absolute difference > 0.05\n",
    "    - Absolute raw EPS < absolute diluted EPS\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for rows with both EPS values present\n",
    "    mask = df['quarterly_raw_eps'].notnull() & df['quarterly_diluted_eps'].notnull()\n",
    "    eps_df = df.loc[mask].copy()\n",
    "\n",
    "    raw_eps = eps_df['quarterly_raw_eps']\n",
    "    diluted_eps = eps_df['quarterly_diluted_eps']\n",
    "\n",
    "    abs_raw = raw_eps.abs()\n",
    "    abs_diluted = diluted_eps.abs()\n",
    "\n",
    "    # Absolute difference\n",
    "    abs_diff = (raw_eps - diluted_eps).abs()\n",
    "\n",
    "    # Percentage difference\n",
    "    eps_df['percentage_difference'] = ((abs_raw - abs_diluted) / abs_raw.replace(0, float('nan'))) * 100\n",
    "\n",
    "    # Conditions\n",
    "    sign_mismatch = (raw_eps * diluted_eps) < 0\n",
    "    pct_difference = (eps_df['percentage_difference'].abs() > threshold_pct) & (abs_diff > 0.02)\n",
    "    raw_smaller = abs_raw < abs_diluted\n",
    "\n",
    "    # Combined condition\n",
    "    anomaly_mask = sign_mismatch | pct_difference | raw_smaller\n",
    "\n",
    "    return eps_df.loc[anomaly_mask, [\n",
    "        'Ticker', 'Query', 'quarterly_raw_eps', 'quarterly_diluted_eps', 'percentage_difference'\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Ticker                      Query  quarterly_raw_eps  \\\n",
      "815    TSLA  TSLA/0001193125-13-212354               0.10   \n",
      "889    WYNN  WYNN/0001174922-23-000105               0.11   \n",
      "1736   VRSN  VRSN/0001014473-18-000028               1.38   \n",
      "5856     GM    GM/0001467858-14-000125               0.08   \n",
      "\n",
      "      quarterly_diluted_eps  percentage_difference  \n",
      "815                    0.00             100.000000  \n",
      "889                   -0.02              81.818182  \n",
      "1736                   1.09              21.014493  \n",
      "5856                   0.06              25.000000  \n",
      "Total anomalies found: 4\n"
     ]
    }
   ],
   "source": [
    "anomalies = flag_eps_anomalies(df_quarterly, threshold_pct=20)\n",
    "print(anomalies.head(50))\n",
    "print(f\"Total anomalies found: {len(anomalies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_10q_filings import extract_eps\n",
    "\n",
    "def reprocess_anomalies(df: pd.DataFrame, anomaly_df: pd.DataFrame, output_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reprocess EPS values for flagged anomaly rows and update the original DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original full DataFrame with EPS data.\n",
    "        anomaly_df (pd.DataFrame): DataFrame of flagged anomalies, must include 'Query', 'Ticker', 'Accession Number'.\n",
    "        output_path (str, optional): If provided, saves the updated DataFrame to this path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated copy of the original DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    for idx, row in anomaly_df.iterrows():\n",
    "        query = row['Query']\n",
    "\n",
    "        # Verify index exists\n",
    "        if idx not in df_copy.index:\n",
    "            print(f\"[WARN] Index {idx} not found in original DataFrame. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Verify query matches at index\n",
    "        if df_copy.loc[idx, 'Query'] != query:\n",
    "            print(f\"[WARN] Query mismatch at index {idx}. Skipping to avoid overwrite.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[INFO] Reprocessing index {idx} | Query: {query}\")\n",
    "        print(f\"Original EPS values -> Basic: {df_copy.loc[idx, 'quarterly_raw_eps']}, \"\n",
    "              f\"Diluted: {df_copy.loc[idx, 'quarterly_diluted_eps']}\")\n",
    "\n",
    "        try:\n",
    "            new_basic, new_diluted = extract_eps(query)\n",
    "            print(f\"Updated EPS values  -> Basic: {new_basic}, Diluted: {new_diluted}\")\n",
    "\n",
    "            df_copy.at[idx, 'quarterly_raw_eps'] = new_basic\n",
    "            df_copy.at[idx, 'quarterly_diluted_eps'] = new_diluted\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract EPS for index {idx}: {e}\")\n",
    "\n",
    "    if output_path:\n",
    "        df_copy.to_csv(\"../../\"+output_path, index=False)\n",
    "        print(f\"[INFO] Updated DataFrame saved to: {output_path}\")\n",
    "\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Reprocessing index 815 | Query: TSLA/0001193125-13-212354\n",
      "Original EPS values -> Basic: 0.1, Diluted: 0.0\n",
      "[INFO] Extracted 2 text blocks, using all for prompt.\n",
      "[INFO] Attempt 1: Using model llama3-70b-8192\n",
      "Prompt tokens: 2095, Completion tokens: 16, Total: 2111\n",
      "[INFO] Received response: basic_eps: 0.10, diluted_eps: 0.00\n",
      "Updated EPS values  -> Basic: 0.1, Diluted: 0.0\n",
      "\n",
      "[INFO] Reprocessing index 889 | Query: WYNN/0001174922-23-000105\n",
      "Original EPS values -> Basic: 0.11, Diluted: -0.02\n",
      "[INFO] Extracted 3 text blocks, using all for prompt.\n",
      "[INFO] Attempt 1: Using model llama3-70b-8192\n",
      "Prompt tokens: 1542, Completion tokens: 16, Total: 1558\n",
      "[INFO] Received response: basic_eps: 0.11, diluted_eps: -0.02\n",
      "Updated EPS values  -> Basic: 0.11, Diluted: -0.02\n",
      "\n",
      "[INFO] Reprocessing index 1736 | Query: VRSN/0001014473-18-000028\n",
      "Original EPS values -> Basic: 1.38, Diluted: 1.09\n",
      "[INFO] Extracted 4 text blocks, using all for prompt.\n",
      "[INFO] Attempt 1: Using model llama3-70b-8192\n",
      "Prompt tokens: 1945, Completion tokens: 16, Total: 1961\n",
      "[INFO] Received response: basic_eps: 1.38, diluted_eps: 1.09\n",
      "Updated EPS values  -> Basic: 1.38, Diluted: 1.09\n",
      "\n",
      "[INFO] Reprocessing index 5856 | Query: GM/0001467858-14-000125\n",
      "Original EPS values -> Basic: 0.08, Diluted: 0.06\n",
      "[INFO] Extracted 5 text blocks, using all for prompt.\n",
      "[INFO] Attempt 1: Using model llama3-70b-8192\n",
      "[INFO] Original prompt length: 10466, Truncated prompt length: 8192, Max tokens for model: 8192\n",
      "Reduced by 21.73%\n",
      "Prompt tokens: 4157, Completion tokens: 16, Total: 4173\n",
      "[INFO] Received response: basic_eps: 0.08, diluted_eps: 0.06\n",
      "Updated EPS values  -> Basic: 0.08, Diluted: 0.06\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '../..Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreprocess_anomalies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43manomaly_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manomalies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPS_DATA_CSV\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[106], line 45\u001b[0m, in \u001b[0;36mreprocess_anomalies\u001b[0;34m(df, anomaly_df, output_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] Failed to extract EPS for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path:\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mdf_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../..\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Updated DataFrame saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_copy\n",
      "File \u001b[0;32m~/miniconda3/envs/apollo/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apollo/lib/python3.10/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apollo/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/miniconda3/envs/apollo/lib/python3.10/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/miniconda3/envs/apollo/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/apollo/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '../..Data'"
     ]
    }
   ],
   "source": [
    "reprocess_anomalies(df=df,\n",
    "                     anomaly_df=anomalies,\n",
    "                     output_path=config.EPS_DATA_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apollo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
