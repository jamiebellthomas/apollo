[INFO] Number of rows in table: 15,549,299

[INFO] Column names of table:
['Unnamed: 0', 'Date', 'Article_title', 'Stock_symbol', 'Url', 'Publisher', 'Author', 'Article', 'Lsa_summary', 'Luhn_summary', 'Textrank_summary', 'Lexrank_summary']

[INFO] Number of None values in each column:
Date: 0
Article_title: 1
Stock_symbol: 9804627
Url: 686
Publisher: 11522656
Author: 14362984
Article: 11809209
Lsa_summary: 13057522
Luhn_summary: 13057521
Textrank_summary: 13057521
Lexrank_summary: 13057521



[INFO] Cleaning the SQLite database...
[INFO] Current row count in news_articles: 87573645
[INFO] New row count in news_articles: 51625
[DONE] News articles database built successfully.

[SUBSET RESULTS]
Subset CPU time:            53.86 s
Subset wall time:           1311.31 s
Subset RSS memory used:     227.41 MB
Subset Peak memory used:    64.55 MB
Subset Disk usage:          -0.06 GB

[PROJECTED TOTAL USAGE]
Total CPU time:             8510.14 s
Total wall time:            207211.75 s
Total RSS memory used:      35934.56 MB
Total Peak memory used:     10200.08 MB
Total Disk usage:           -8.93 GB

To do:
Seperate new stories where appropriate, currently we have all related tickers for each story, what we need to do now is to go through each of these
related tickers and see how they fit into the article as a whole, some related tickers are just independent facts chucked into the articles 
and some relate directly to the primary ticker. We need an big LLM pipeline to go through this and do this seperation, making a new db for stories
FOR EXAMPLE:
If we have an AAPL story talking about a partnership with NVDA and providing a general updated on AMZN and EBAY, we need to do the following
Convert:
[Date], AAPL, [Article], [NVDA,AMZN,EBAY]

To:
[Date], [AAPL, NVDA], [Article]
[Date], [AMZN], [Fact]
[Date], [EBAY], [Fact]

Much cleaner representation for the knowledge graph. plus means we don't have rogue links. The problem with the first representation is that
(in this case) AMZN and EBAY would be linked to AAPL in the KG, despite the article not really creating any real link there
