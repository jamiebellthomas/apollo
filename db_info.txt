[INFO] Number of rows in table: 15,549,299

[INFO] Column names of table:
['Unnamed: 0', 'Date', 'Article_title', 'Stock_symbol', 'Url', 'Publisher', 'Author', 'Article', 'Lsa_summary', 'Luhn_summary', 'Textrank_summary', 'Lexrank_summary']

[INFO] Number of None values in each column:
Date: 0
Article_title: 1
Stock_symbol: 9804627
Url: 686
Publisher: 11522656
Author: 14362984
Article: 11809209
Lsa_summary: 13057522
Luhn_summary: 13057521
Textrank_summary: 13057521
Lexrank_summary: 13057521



[INFO] Cleaning the SQLite database...
[INFO] Current row count in news_articles: 87573645
[INFO] New row count in news_articles: 51625
[DONE] News articles database built successfully.

[SUBSET RESULTS]
Subset CPU time:            53.86 s
Subset wall time:           1311.31 s
Subset RSS memory used:     227.41 MB
Subset Peak memory used:    64.55 MB
Subset Disk usage:          -0.06 GB

[PROJECTED TOTAL USAGE]
Total CPU time:             8510.14 s
Total wall time:            207211.75 s
Total RSS memory used:      35934.56 MB
Total Peak memory used:     10200.08 MB
Total Disk usage:           -8.93 GB

To do:
Seperate new stories where appropriate, currently we have all related tickers for each story, what we need to do now is to go through each of these
related tickers and see how they fit into the article as a whole, some related tickers are just independent facts chucked into the articles 
and some relate directly to the primary ticker. We need an big LLM pipeline to go through this and do this seperation, making a new db for stories
FOR EXAMPLE:
If we have an AAPL story talking about a partnership with NVDA and providing a general updated on AMZN and EBAY, we need to do the following
Convert:
[Date], AAPL, [Article], [NVDA,AMZN,EBAY]

To:
[Date], [AAPL, NVDA], [Article]
[Date], [AMZN], [Fact]
[Date], [EBAY], [Fact]

Much cleaner representation for the knowledge graph. plus means we don't have rogue links. The problem with the first representation is that
(in this case) AMZN and EBAY would be linked to AAPL in the KG, despite the article not really creating any real link there


-----

### Notes: General KG Encoding Strategy

* **Nodes:**

  * Two distinct node types:

    * **Fact nodes:**

      * One per extracted fact.
      * Node feature: dense vector embedding of the fact’s text, computed using **FinBERT**.
    * **Company nodes:**

      * One per company/ticker.
      * Node feature: quantitative financial metrics of the company (e.g., EPS, revenue, etc.).

* **Edges:**

  * Edges only connect **fact nodes → company nodes**.
  * No direct company–company edges, which avoids introducing potentially spurious links.
  * Every fact node may connect to multiple company nodes (if multiple tickers are mentioned in the fact).

* **Edge features:**

  * **Date score:**

    * Encodes recency of the fact relative to prediction time.
    * Computed using a decay function, e.g.:

      ```
      date_score = exp(-λ * Δt)
      ```

      where `Δt` is the time difference and `λ` is a tunable decay parameter.
  * **Sentiment:**

    * The sentiment of the fact, as extracted.
    * Use raw or normalized to range `[-1, 1]`.

* **Filtering:**

  * Facts with near-zero sentiment can optionally be excluded from the KG (since they carry little directional signal).

* **Event type:**

  * Rather than using raw `event_type` strings, cluster them into canonical categories.
  * Use a clean mapping of each fact’s `event_type` to its cluster ID as an additional feature.

* **Why this design makes sense:**

  * Heterogeneous graph structure: two distinct node types, meaningful edge features.
  * Avoids noisy direct company–company edges while still allowing information flow via shared facts.
  * Fact nodes encode unstructured text signals; company nodes encode structured numeric signals.
  * Temporal and sentiment signals are explicitly embedded in the edge.

---


To do:

- 10-K extraction pipeline, try and get Q1,2,3,&4 EPS data if possible, get - DONE/Pipeline set up, issues running it currnetly for some reason
- FinBERT extraction, function is set up for this, just need to make a function that runs it over facts_output.jsonl
- Model exponential decay DONE

- Read papers to see if there is a gold standard for H-GNN formatting NO NEED, PyTorch will handle all this
- KG construction, Intermediate representation completed, encoding completed, now we just need to load it into a PyTorch H-GNN 
- Start researching Heterogeneous GNN stuff/relook at notes from DGL module - this will be a really good bit of maths to talk about in the report 

- Labels for dataset, for this we need predicted EPS and a way to determine PEAD from pricing data (Maybe this can be done via combining short-term and long-term)
moving averages.

- Encoding stage of SubGraph object complete, now we need a data loader method that encode the output that aggregates SubGraph encodings into batches for model 
consumption

- Revisit whether only using cluster centroids is the best idea - we may loose alot of detail here, and if its similar to centroid encoding has similar meaning anyway
we can still toggle different article type by the event_type centroid. 

- Compare to standard numerical methods (make these and run them)







Report sections to write:
- Company selection and meta data
- Pricing data
- EPS data (extraction and eval)
- News articles source, extraction, clearning, analysis+eval
- Articles -> facts + notebook eval (to be completed)
- Discuss LLM selection, use benchmark scores on the LLM model cardsin both general and long context (even though we'll use LLama3)
BASICALLY MAKE SURE THAT EVERY LINE OF CODE HAS SOME KIND OF REFERENCE IN REPORT
- KG definition, background maths and out implementation, use as many fancy symbols as possible


