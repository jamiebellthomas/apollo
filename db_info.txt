[INFO] Number of rows in table: 15,549,299

[INFO] Column names of table:
['Unnamed: 0', 'Date', 'Article_title', 'Stock_symbol', 'Url', 'Publisher', 'Author', 'Article', 'Lsa_summary', 'Luhn_summary', 'Textrank_summary', 'Lexrank_summary']

[INFO] Number of None values in each column:
Date: 0
Article_title: 1
Stock_symbol: 9804627
Url: 686
Publisher: 11522656
Author: 14362984
Article: 11809209
Lsa_summary: 13057522
Luhn_summary: 13057521
Textrank_summary: 13057521
Lexrank_summary: 13057521



[INFO] Cleaning the SQLite database...
[INFO] Current row count in news_articles: 87573645
[INFO] New row count in news_articles: 51625
[DONE] News articles database built successfully.

[SUBSET RESULTS]
Subset CPU time:            53.86 s
Subset wall time:           1311.31 s
Subset RSS memory used:     227.41 MB
Subset Peak memory used:    64.55 MB
Subset Disk usage:          -0.06 GB

[PROJECTED TOTAL USAGE]
Total CPU time:             8510.14 s
Total wall time:            207211.75 s
Total RSS memory used:      35934.56 MB
Total Peak memory used:     10200.08 MB
Total Disk usage:           -8.93 GB

To do:
Seperate new stories where appropriate, currently we have all related tickers for each story, what we need to do now is to go through each of these
related tickers and see how they fit into the article as a whole, some related tickers are just independent facts chucked into the articles 
and some relate directly to the primary ticker. We need an big LLM pipeline to go through this and do this seperation, making a new db for stories
FOR EXAMPLE:
If we have an AAPL story talking about a partnership with NVDA and providing a general updated on AMZN and EBAY, we need to do the following
Convert:
[Date], AAPL, [Article], [NVDA,AMZN,EBAY]

To:
[Date], [AAPL, NVDA], [Article]
[Date], [AMZN], [Fact]
[Date], [EBAY], [Fact]

Much cleaner representation for the knowledge graph. plus means we don't have rogue links. The problem with the first representation is that
(in this case) AMZN and EBAY would be linked to AAPL in the KG, despite the article not really creating any real link there


-----

### Notes: General KG Encoding Strategy

* **Nodes:**

  * Two distinct node types:

    * **Fact nodes:**

      * One per extracted fact.
      * Node feature: dense vector embedding of the fact’s text, computed using **FinBERT**.
    * **Company nodes:**

      * One per company/ticker.
      * Node feature: quantitative financial metrics of the company (e.g., EPS, revenue, etc.).

* **Edges:**

  * Edges only connect **fact nodes → company nodes**.
  * No direct company–company edges, which avoids introducing potentially spurious links.
  * Every fact node may connect to multiple company nodes (if multiple tickers are mentioned in the fact).

* **Edge features:**

  * **Date score:**

    * Encodes recency of the fact relative to prediction time.
    * Computed using a decay function, e.g.:

      ```
      date_score = exp(-λ * Δt)
      ```

      where `Δt` is the time difference and `λ` is a tunable decay parameter.
  * **Sentiment:**

    * The sentiment of the fact, as extracted.
    * Use raw or normalized to range `[-1, 1]`.

* **Filtering:**

  * Facts with near-zero sentiment can optionally be excluded from the KG (since they carry little directional signal).

* **Event type:**

  * Rather than using raw `event_type` strings, cluster them into canonical categories.
  * Use a clean mapping of each fact’s `event_type` to its cluster ID as an additional feature.

* **Why this design makes sense:**

  * Heterogeneous graph structure: two distinct node types, meaningful edge features.
  * Avoids noisy direct company–company edges while still allowing information flow via shared facts.
  * Fact nodes encode unstructured text signals; company nodes encode structured numeric signals.
  * Temporal and sentiment signals are explicitly embedded in the edge.

---


To do:

- 10-K extraction pipeline, try and get Q1,2,3,&4 EPS data if possible, get - DONE/Pipeline set up, issues running it currnetly for some reason
- FinBERT extraction, function is set up for this, just need to make a function that runs it over facts_output.jsonl
- Model exponential decay DONE

- Read papers to see if there is a gold standard for H-GNN formatting NO NEED, PyTorch will handle all this
- KG construction, Intermediate representation completed, encoding completed, now we just need to load it into a PyTorch H-GNN 
- Start researching Heterogeneous GNN stuff/relook at notes from DGL module - this will be a really good bit of maths to talk about in the report 

- Labels for dataset, for this we need predicted EPS and a way to determine PEAD from pricing data (Maybe this can be done via combining short-term and long-term)
moving averages.

- Encoding stage of SubGraph object complete, now we need a data loader method that encode the output that aggregates SubGraph encodings into batches for model 
consumption

- Revisit whether only using cluster centroids is the best idea - we may loose alot of detail here, and if its similar to centroid encoding has similar meaning anyway
we can still toggle different article type by the event_type centroid. 

- Compare to standard numerical methods (make these and run them)

- Deduplicate via cosine distance between text embeddings (combined with event_type, date and sentiment)

- IMPORTANT: FOR FEATURE LABEL EVALUATION WERE JUST GOING TO USE THE MEDIUM PERIOD NOT THE INITIAL SPIKE, LOOK AT THE COOL PLOT WE MADE PROVING PEAD EFFECT
THERE IS NO SHORT TERM SPIKE FOR WHATEVER REASON, BUT WE HAVE PROVED PEAD HAPPENS FOR POSITIVE SURPRISES. LABEL = POSITIVE CAR OVER MEDIUM PERIOD





Report sections to write:
- Company selection and meta data
- Pricing data
- EPS data (extraction and eval)
- News articles source, extraction, clearning, analysis+eval
- Articles -> facts + notebook eval (to be completed)
- Discuss LLM selection, use benchmark scores on the LLM model cardsin both general and long context (even though we'll use LLama3)
BASICALLY MAKE SURE THAT EVERY LINE OF CODE HAS SOME KIND OF REFERENCE IN REPORT
- KG definition, background maths and out implementation, use as many fancy symbols as possible
- Data leakage prevention - A fact can relate to multiple tickers and therefore could appear in the test AND train sets 

Before: 6124 lines in subgraphs.jsonl
Now: 5888 lines in subgraphs.jsonl


docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HUGGING_FACE_HUB_TOKEN=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" \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model meta-llama/Llama-3.3-70B-Instruct




What each feature is (and why it’s here)
Momentum / returns
cumret_20, cumret_60, cumret_252 — Sum of log returns over 1m/3m/1y. Captures recent trend and long-term trajectory.
mom_10_minus_60 — Short-over-long momentum spread; highlights accelerations or decelerations before earnings.
Moving-average gaps
ma_gap_20, ma_gap_60 — Distance of price from 1m/3m average at day 0. A proxy for “overbought/oversold” relative to recent baseline.
Up-day share
up_pct_20, up_pct_60 — Fraction of positive return days. A robust momentum proxy less sensitive to a single outlier move.
Volatility / downside risk / drawdowns
vol_20, vol_60 — Realized volatility; captures regime shifts in risk leading into earnings.
downside_vol_20, downside_vol_60 — Penalizes negative shocks more; useful if bad-news fear dominates.
max_dd_60, max_dd_252 — Max drawdown in 3m/1y. Captures fragility/overhangs that can affect post-earnings behavior.
Market-adjusted (vs SPY)
abn_sum_20 — Sum of daily abnormal returns over 20d; isolates stock-specific drift from market noise pre-event.
beta_60, alpha_60, resid_vol_60 — OLS of stock on SPY over 60d: sensitivity (beta), average excess (alpha), idiosyncratic risk (residual vol). These affect how much of a move is likely firm-specific.
corr_60 — Correlation with market; low correlation can make firm news move the stock more distinctly.
Trend (price & abnormal)
slope_price_60 — Linear trend of log-price over 60d (per day). A clean momentum slope.
slope_car_60_bps — Linear trend of CAR (cum abnormal return) over 60d, in bps/day. Direct proxy for pre-event stock-specific drift.
Pre-event leakage proxy
CAR_pre20 — Sum of abnormal returns from −20 to −1. Negative-surprise names often drift down into prints; this measures that anticipation/leakage directly.
Technical
RSI_14 — 14-day RSI (simple version). Popular overbought/oversold indicator; often correlates with near-term reversals or momentum continuation.
MACD_diff, MACD_signal — Momentum oscillator and its signal line at day 0; captures medium-term trend state.
52-week positioning
pct_to_52w_high, pct_from_52w_low — Where the stock sits within its 1-year range. Extremes can moderate or amplify post-earnings reactions (mean reversion vs breakout).


Link prediction algorithm

Change the graphs to show CAR from day 15 too?? Ask GPT
Get plots of average car for both architectures and on the same plot
heterognn Struggled wigh higher learning rates?

Ok, new architecture I want to try tomorrow is edges aren't bidirectional instead everything flows into the primary ticker - Secondary ticker->Fact->Primary Ticker